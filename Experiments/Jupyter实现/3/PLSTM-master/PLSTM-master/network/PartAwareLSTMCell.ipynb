{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load PartAwareLSTMCell.py\n",
    "# !/usr/bin/env python\n",
    "\n",
    "__author__ = 'FesianXu'\n",
    "__date__ = 2018 / 3 / 20\n",
    "__version__ = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell_impl import RNNCell, LSTMStateTuple\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops, math_ops\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartAwareLSTMCell(RNNCell):\n",
    "  '''\n",
    "  The implement of paper <NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis>\n",
    "  Part-Aware LSTM Cell\n",
    "  '''\n",
    "  def __init__(self,num_units,forget_bias=1.0,state_is_tuple=True,activation=None,reuse=None):\n",
    "    # here num_units has to be times of 5\n",
    "    assert num_units % 5 == 0\n",
    "\n",
    "    super(PartAwareLSTMCell, self).__init__(_reuse=reuse)\n",
    "    if not state_is_tuple:\n",
    "      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "                   \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "    self._num_units = int(num_units/5)\n",
    "    self._forget_bias = forget_bias\n",
    "    self._state_is_tuple = state_is_tuple\n",
    "    self._activation = activation or math_ops.tanh\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    cs_size = self._num_units * 5\n",
    "    return (LSTMStateTuple(cs_size, 5*self._num_units)\n",
    "            if self._state_is_tuple else 2 * self._num_units)\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units * 5\n",
    "\n",
    "\n",
    "  def call(self, skel_inputs, state):\n",
    "    '''\n",
    "    here inputs with the shape of (batch_size, feat_dim)\n",
    "    in kinect 2.0, feat_dim is 25*3 = 75\n",
    "    for five part of a skeleton body.\n",
    "    (head, r_arm, l_arm, r_leg, l_leg)\n",
    "    divide config:\n",
    "    head:  [ 3, 4,              1,2,21]\n",
    "    r_arm: [ 5, 6, 7, 8,22,23,  1,2,21]\n",
    "    l_arm: [ 9,10,11,12,24,25,  1,2,21]\n",
    "    r_leg: [13,14,15,16,        1,2,21]\n",
    "    l_leg: [17,18,19,20,        1,2,21]\n",
    "\n",
    "    state: LSTMStateTuple with the format of (Tensor(c1, c2, ..., c5), Tensor(h))\n",
    "    '''\n",
    "\n",
    "    sigmoid = math_ops.sigmoid\n",
    "    tanh = math_ops.tanh\n",
    "\n",
    "    if self._state_is_tuple:\n",
    "      cs, h = state\n",
    "    else:\n",
    "      cs, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n",
    "    # split he state into c and h\n",
    "    # here cs mean c1 to c5, where each part means a part of body, cs is also a list or turple\n",
    "\n",
    "    # split the cs into 5 parts\n",
    "    cs = array_ops.split(cs, num_or_size_splits=5, axis=1)\n",
    "\n",
    "    divide_config = {\n",
    "      'head':  ( 3, 4,              1,2,21),\n",
    "      'r_arm': ( 5, 6, 7, 8,22,23,  1,2,21),\n",
    "      'l_arm': ( 9,10,11,12,24,25,  1,2,21),\n",
    "      'r_leg': ( 13,14,15,16,       1,2,21),\n",
    "      'l_leg': ( 17,18,19,20,       1,2,21)\n",
    "    }\n",
    "    # assert skel_inputs.shape[1] == 75\n",
    "\n",
    "    reshaped_input = array_ops.reshape(skel_inputs, shape=(-1, 25, 3))\n",
    "    head_joints = [reshaped_input[:, each-1, :] for each in divide_config['head']]\n",
    "    r_arm_joints = [reshaped_input[:, each-1, :] for each in divide_config['r_arm']]\n",
    "    l_arm_joints = [reshaped_input[:, each-1, :] for each in divide_config['l_arm']]\n",
    "    r_leg_joints = [reshaped_input[:, each-1, :] for each in divide_config['r_leg']]\n",
    "    l_leg_joints = [reshaped_input[:, each-1, :] for each in divide_config['l_leg']]\n",
    "\n",
    "    body_list = [head_joints, r_arm_joints, l_arm_joints, r_leg_joints, l_leg_joints]\n",
    "\n",
    "    body_list = ops.convert_n_to_tensor(body_list)\n",
    "\n",
    "    for ind, each in enumerate(body_list):\n",
    "      tmp = array_ops.transpose(each, perm=(1,0,2))\n",
    "      batch_size = int(tmp.shape[0])\n",
    "      body_list[ind] = array_ops.reshape(tmp, shape=(batch_size, -1))\n",
    "\n",
    "    o_all_skel = _linear([body_list[0],\n",
    "                          body_list[1],\n",
    "                          body_list[2],\n",
    "                          body_list[3],\n",
    "                          body_list[4],\n",
    "                          h], # here 111 + h_size\n",
    "                         5 * self._num_units, True)\n",
    "    o_all_skel = sigmoid(o_all_skel)\n",
    "    new_c_list = []\n",
    "    for ind, each_part in enumerate(body_list):\n",
    "      concat_p = _linear([each_part, h],\n",
    "                         3 * self._num_units,\n",
    "                         weight_name='weight_%d' % ind,\n",
    "                         bias_name='bias_%d' % ind,\n",
    "                         bias=True)\n",
    "      ip, fp, gp = array_ops.split(value=concat_p, num_or_size_splits=3, axis=1)\n",
    "      ip, fp, gp = sigmoid(ip), sigmoid(fp), tanh(gp)\n",
    "      new_c = cs[ind] * (fp+self._forget_bias) + ip * gp\n",
    "      new_c_list.append(new_c)\n",
    "\n",
    "\n",
    "    new_c_tensors = array_ops.concat(new_c_list, axis=1)\n",
    "    new_h = o_all_skel * tanh(array_ops.concat(new_c_list, 1))\n",
    "\n",
    "    if self._state_is_tuple:\n",
    "      new_state = LSTMStateTuple(new_c_tensors, new_h)\n",
    "    else:\n",
    "      new_state = array_ops.concat([new_c_tensors, new_h], 1)\n",
    "\n",
    "    return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "\n",
    "_BIAS_VARIABLE_NAME = \"bias\"\n",
    "_WEIGHTS_VARIABLE_NAME = \"kernel\"\n",
    "\n",
    "\n",
    "def _linear(args,\n",
    "            output_size,\n",
    "            bias,\n",
    "            weight_name=_WEIGHTS_VARIABLE_NAME,\n",
    "            bias_name=_BIAS_VARIABLE_NAME,\n",
    "            bias_initializer=None,\n",
    "            kernel_initializer=None):\n",
    "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "\n",
    "  Args:\n",
    "    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    bias: boolean, whether to add a bias term or not.\n",
    "    bias_initializer: starting value to initialize the bias\n",
    "      (default is all zeros).\n",
    "    kernel_initializer: starting value to initialize the weight.\n",
    "\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  \"\"\"\n",
    "  if args is None or (nest.is_sequence(args) and not args):\n",
    "    raise ValueError(\"`args` must be specified\")\n",
    "  if not nest.is_sequence(args):\n",
    "    args = [args]\n",
    "\n",
    "  # Calculate the total size of arguments on dimension 1.\n",
    "  total_arg_size = 0\n",
    "  shapes = [a.get_shape() for a in args]\n",
    "  for shape in shapes:\n",
    "    if shape.ndims != 2:\n",
    "      raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n",
    "    if shape[1].value is None:\n",
    "      raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
    "                       \"but saw %s\" % (shape, shape[1]))\n",
    "    else:\n",
    "      total_arg_size += shape[1].value\n",
    "\n",
    "  dtype = [a.dtype for a in args][0]\n",
    "\n",
    "  # Now the computation.\n",
    "  scope = vs.get_variable_scope()\n",
    "  with vs.variable_scope(scope) as outer_scope:\n",
    "    weights = vs.get_variable(\n",
    "        weight_name, [total_arg_size, output_size],\n",
    "        dtype=dtype,\n",
    "        initializer=kernel_initializer)\n",
    "\n",
    "    # if the args is a single tensor then matmul it with weight\n",
    "    # if the args is a list of tensors then concat them in axis of 1 and matmul\n",
    "    if len(args) == 1:\n",
    "      res = math_ops.matmul(args[0], weights)\n",
    "    else:\n",
    "      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n",
    "    if not bias:\n",
    "      return res\n",
    "    with vs.variable_scope(outer_scope) as inner_scope:\n",
    "      inner_scope.set_partitioner(None)\n",
    "      if bias_initializer is None:\n",
    "        bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n",
    "      biases = vs.get_variable(\n",
    "          bias_name, [output_size],\n",
    "          dtype=dtype,\n",
    "          initializer=bias_initializer)\n",
    "    return nn_ops.bias_add(res, biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
