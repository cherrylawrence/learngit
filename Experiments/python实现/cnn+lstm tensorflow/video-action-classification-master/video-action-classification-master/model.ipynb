{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import vgg19\n",
    "import cnnm\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Failed to interpret file 'vgg19.py' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: could not find MARK",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-496b5041be3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;31m# build VGG19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mvgg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVgg19\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vgg19.py\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspatial_video\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mvgg_fc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_spatial_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4096\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\python实现\\cnn+lstm tensorflow\\video-action-classification-master\\video-action-classification-master\\vgg19.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vgg19_npy_path, trainable, dropout)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvgg19_npy_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvgg19_npy_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg19_npy_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m                 raise IOError(\n\u001b[1;32m--> 429\u001b[1;33m                     \"Failed to interpret file %s as a pickle\" % repr(file))\n\u001b[0m\u001b[0;32m    430\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Failed to interpret file 'vgg19.py' as a pickle"
     ]
    }
   ],
   "source": [
    "vdir = \"data/\"\n",
    "optical_flow_dir = vdir+\"optical-flow/\"\n",
    "res_dir = vdir+\"resized/\"\n",
    "\n",
    "trainlist = \"trainlist01.txt\"\n",
    "if not os.path.isfile(trainlist):\n",
    "    print(\"Training set description not found!\")\n",
    "    exit()\n",
    "trainlist = open(trainlist, \"r\").readlines()\n",
    "training_set_length = len(trainlist)\n",
    "training_set_offset = 0\n",
    "\n",
    "def get_video(file, color=True):\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if color:\n",
    "        frames = np.zeros((num_frames, 224, 224, 3))\n",
    "    else:\n",
    "        frames = np.zeros((num_frames, 224, 224))\n",
    "\n",
    "    # load video into numpy array in the following format:\n",
    "    # ar = [num_frames, 224, 224, 3]\n",
    "    k=0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if not color:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames[k] = frame\n",
    "        k = k+1\n",
    "\n",
    "    return num_frames, frames\n",
    "\n",
    "def get_data(L):\n",
    "    global training_set_offset, training_set_length\n",
    "\n",
    "    file, label = trainlist[training_set_offset].split()\n",
    "\n",
    "    # if the file listed in training set doesn't exist, remove it from training set and continue on to next one\n",
    "    # mostly irrelevant for full-blown training but helpful for training on small subset of training set\n",
    "    while not (os.path.isfile(res_dir+file) and os.path.isfile(optical_flow_dir+file)):\n",
    "        del trainlist[training_set_offset]\n",
    "        training_set_length = training_set_length - 1\n",
    "        if training_set_offset >=- training_set_length:\n",
    "            training_set_offset = 0\n",
    "        file, label = trainlist[training_set_offset].split()\n",
    "\n",
    "    training_set_offset += 1\n",
    "    if training_set_offset >= training_set_length :\n",
    "        training_set_offset = 0\n",
    "\n",
    "    # read spatial video\n",
    "    _, spatial_frames = get_video(res_dir+file, color=True)\n",
    "    \n",
    "    # read optical flow video\n",
    "    num_motion_frames, motion_frames = get_video(optical_flow_dir+file, color=False)\n",
    "\n",
    "    # stack optical flow frames efficiently using numpy's stride_tricks method\n",
    "    sizeof_int32 = np.dtype(np.int32).itemsize\n",
    "\n",
    "    # stride by 1 frame with each time window and stride by 1 frame within frames in each time window\n",
    "    # let A, B, C... be consequent frames and L=3\n",
    "    # transformation can be represented as [A, B, C, ...] --> [[A, B, C], [B, C, D], [C, D, E], ...]\n",
    "    stacked_motion_frames = np.lib.stride_tricks.as_strided(motion_frames, (num_motion_frames-L+1, L, 224, 224),\n",
    "                                        (sizeof_int32*224*224, sizeof_int32*224*224, sizeof_int32*224, sizeof_int32))\n",
    "    stacked_motion_frames = np.reshape(stacked_motion_frames, (num_motion_frames-L+1, 224, 224, L))\n",
    "\n",
    "    return spatial_frames, stacked_motion_frames, label\n",
    "\n",
    "def initialize_fc(name, shape, mean=0.0, dev=1e-3, scope=None):\n",
    "    assert len(shape) == 2\n",
    "    weight = tf.get_variable(name+\"_weights\", shape, dtype=tf.float32,\n",
    "                                  initializer=tf.zeros_initializer())\n",
    "    bias = tf.get_variable(name+\"_biases\", shape[-1], dtype=tf.float32,\n",
    "                                  initializer=tf.zeros_initializer())\n",
    "    return weight, bias\n",
    "\n",
    "def fc_layer(name, prev, shape, gate=\"relu\", mean=0.0, dev=1e-3):\n",
    "    with tf.variable_scope(name):\n",
    "        weights, biases = initialize_fc(name, shape, mean, dev)\n",
    "        output = tf.matmul(prev, weights)+biases\n",
    "\n",
    "        if gate == \"relu\":\n",
    "            output = tf.nn.relu(output)\n",
    "        elif gate == \"tanh\":\n",
    "            output = tf.tanh(output)\n",
    "        elif gate == \"sigmoid\":\n",
    "            output = tf.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# L : height of stacked optical flows as input to temporal learning CNNs\n",
    "L = 10\n",
    "# C : number of classes\n",
    "C = 101\n",
    "# learning rates for optimizers\n",
    "lr_proximal_gradient = 0.001\n",
    "lr_gradient = 0.001\n",
    "# lmbd1, lmbd2 : l1, l2 norm reg. constants for proximal gradient respectively\n",
    "lmbd1 = lmbd2 = 1e-5\n",
    "# number of steps to train\n",
    "num_steps = 100000\n",
    "\n",
    "# create and train the model\n",
    "with tf.Session() as sess:\n",
    "    spatial_video = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    stacked_flow = tf.placeholder(tf.float32, [None, 224, 224, L])\n",
    "    labels = tf.placeholder(tf.int32, [1])\n",
    "\n",
    "    num_spatial_frames = tf.shape(spatial_video)[0]\n",
    "    num_flow_stacks = tf.shape(stacked_flow)[0]\n",
    "\n",
    "    # build VGG19\n",
    "    vgg = vgg19.Vgg19(\"vgg19.py\")\n",
    "    vgg.build(spatial_video)\n",
    "    vgg_fc = tf.reshape(vgg.relu6, [1, num_spatial_frames, 4096])\n",
    "\n",
    "    # build spatial LSTM network\n",
    "    with tf.variable_scope(\"spatial_lstm\"):\n",
    "        lstm_stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(1024, state_is_tuple=True),\n",
    "                                                  tf.contrib.rnn.BasicLSTMCell(512, state_is_tuple=True)], state_is_tuple=True)\n",
    "        spatial_lstm = tf.nn.dynamic_rnn(lstm_stack, vgg_fc, dtype=tf.float32, time_major=False)\n",
    "        # method 1: aggregate all time frames\n",
    "        # spatial_features = tf.reduce_sum(spatial_lstm[0][0], axis=0, keep_dims=True)\n",
    "        # method 2: use the last time frame features as video level features\n",
    "        spatial_features = tf.reshape(spatial_lstm[0][0][-1], [1, 512])\n",
    "\n",
    "    # build CnnM\n",
    "    cnnm2048 = cnnm.CnnM(L)\n",
    "    cnnm2048.build(stacked_flow)\n",
    "    cnnm_fc = cnnm2048.relu1\n",
    "    cnnm_fc = tf.reshape(cnnm_fc, [1, num_flow_stacks, 4096])\n",
    "\n",
    "    # build motion LSTM network\n",
    "    with tf.variable_scope(\"motion_lstm\"):\n",
    "        lstm_stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(1024, state_is_tuple=True),\n",
    "                                                  tf.contrib.rnn.BasicLSTMCell(512, state_is_tuple=True)], state_is_tuple=True)\n",
    "        motion_lstm = tf.nn.dynamic_rnn(lstm_stack, cnnm_fc, dtype=tf.float32, time_major=False)\n",
    "        #motion_features = tf.reduce_sum(motion_lstm[0][0], axis=0, keep_dims=True)\n",
    "        motion_features = tf.reshape(motion_lstm[0][0][-1], [1, 512])\n",
    "\n",
    "    # aggregate frame level CNN outputs to get video level CNN features\n",
    "    with tf.variable_scope(\"regularized_fusion_network\"):\n",
    "        spatial_video_level_features = tf.reduce_mean(vgg.relu6, axis=0, keep_dims=True)\n",
    "        motion_video_level_features = tf.reduce_mean(cnnm2048.relu1, axis=0, keep_dims=True)\n",
    "\n",
    "    fc_spatial = fc_layer(\"fc_spatial\", spatial_video_level_features, (4096, 200), \"relu\", 0.0, 0.001)\n",
    "    fc_motion = fc_layer(\"fc_motion\", motion_video_level_features, (4096, 200), \"relu\", 0.0, 0.001)\n",
    "\n",
    "    with tf.variable_scope(\"regularized_fusion_network\"):\n",
    "        fusion_layer_input = fc_spatial + fc_motion\n",
    "    # regularized fusion layer\n",
    "    fusion_layer = fc_layer(\"fusion_layer\", fusion_layer_input, (200, 200), \"relu\", 0.0, 0.001)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # we can calculate prediction scores of temporal model and regularized fusion network separetely\n",
    "    # and then combine them, but in order to work easier with tensorflow,\n",
    "    # I'm going to concatenate all feature vectors and have one softmax layer\n",
    "    # note that, since my proposed method is a generalization of the first method, \n",
    "    # (when some of the weights are zero)\n",
    "    # it will converge to a good solution, altough, there may be a performance hit,\n",
    "    # due to the computational cost of training one large layer compared to two smaller ones\n",
    "    \n",
    "    ############################################\n",
    "\n",
    "    # spatial_lstm_pred = fc_layer(\"fusion\", spatial_features, (512, C), \"linear\", 0.0, 0.001)\n",
    "    # motion_lstm_pred = fc_layer(\"fusion\", motion_features, (512, C), \"linear\", 0.0, 0.001)\n",
    "    # fusion_layer_pred = fc_layer(\"softmax\", fusion, (200, C), \"linear\", 0.0, 0.001)\n",
    "    # to follow the paper verbatim, feed these three layers into softmax function \n",
    "    # and linearly combine their outputs to get the final prediction scores\n",
    "\n",
    "    logits = tf.concat([spatial_features, motion_features, fusion_layer], axis=1)\n",
    "    logits = fc_layer(\"softmax\", logits, (1224, C), \"linear\", 0.0, 0.001)\n",
    "\n",
    "    # define loss function\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "\n",
    "    # gradient optimizer on global variables\n",
    "    gradient_optimizer = tf.train.GradientDescentOptimizer(lr_gradient)\n",
    "    gd_opt = gradient_optimizer.minimize(loss)\n",
    "\n",
    "    # proximal gradient optimizer on fusion layer weights as defined in original paper\n",
    "    proximal_gradient_optimizer = tf.train.ProximalGradientDescentOptimizer(lr_proximal_gradient, lmbd1, lmbd2)\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        fusion_layer_weights = tf.get_variable(\"fusion_layer/fusion_layer_weights\")\n",
    "        fusion_layer_biases = tf.get_variable(\"fusion_layer/fusion_layer_biases\")\n",
    "        p_grads = proximal_gradient_optimizer.compute_gradients(loss, var_list=[fusion_layer_weights, fusion_layer_biases])\n",
    "        pgd_opt = proximal_gradient_optimizer.apply_gradients(p_grads, global_step=None)\n",
    "\n",
    "    # to conserve memeory, pre-trained Vgg19 weights are fed into the model using placeholders\n",
    "    # This reduces the amount of memory needed in this step to about a third\n",
    "    feed_dict = {}\n",
    "    feed_dict.update(vgg.var_dict)\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict=feed_dict)\n",
    "    \n",
    "    feed_dict[spatial_video] = feed_dict[stacked_flow] = feed_dict[labels] = None\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        spatial_frames, stacked_motion_frames, label = get_data(L)\n",
    "        feed_dict[spatial_video] = spatial_frames\n",
    "        feed_dict[stacked_flow] = stacked_motion_frames\n",
    "        feed_dict[labels] = np.array([label])\n",
    "        _, _, l = sess.run([gd_opt, pgd_opt, loss], feed_dict=feed_dict)\n",
    "        print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
